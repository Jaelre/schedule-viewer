# Telemetry Deep-Dive

This document explains how the telemetry client and Worker endpoint cooperate to
emit usage events. It supplements the high-level summary in the
[README](../README.md#telemetry) and provides operational notes for tuning,
storage, and verification.

## Event schema

Each browser event is enqueued as a JSON object with the following structure:

```json
{
  "name": "shift_grid.view",
  "timestamp": "2025-01-18T09:15:32.904Z",
  "sequence": 17,
  "context": {
    "ym": "2025-02",
    "density": "compact"
  },
  "user": {
    "id": "33935",
    "role": "clinician"
  },
  "device": {
    "locale": "it-IT",
    "viewport": {
      "width": 1440,
      "height": 900
    }
  }
}
```

- `name` *(string)* – Stable identifier for the action being logged.
- `timestamp` *(ISO 8601 string)* – When the event was generated in the browser.
- `sequence` *(number)* – Monotonic counter generated by the client helper so the
  Worker can reason about ordering inside a batch.
- `context` *(object)* – Arbitrary payload describing feature-level state. Keep
  keys snake_case and values serialisable as JSON.
- `user` *(object)* – Optional user metadata; redact direct identifiers unless
  the downstream storage is already protected.
- `device` *(object)* – Snapshot of device details (locale, viewport, user agent
  fragment). This block is optional to keep payloads lightweight.

## Batching and flush thresholds

The client helper appends events to an in-memory queue and evaluates the
following flush conditions:

| Trigger | Description | Config | Default |
| --- | --- | --- | --- |
| Batch size | Flush when the queue reaches the batch limit. | `TELEMETRY_MAX_BATCH_SIZE` (Worker) | 20 |
| Interval | Flush on a cadence even if the queue is not full. | `TELEMETRY_FLUSH_INTERVAL_MS` (Worker) | 5000 ms |
| Page lifecycle | Flush immediately when the document is hidden, the tab closes, or the browser fires `beforeunload`. | n/a | Always on |

The Worker returns HTTP 202 upon successful ingestion. Non-2xx responses leave
the queue intact and the helper retries on the next interval tick so that short
outages do not drop data.

## Storage and logging destinations

Incoming batches are handled by the Worker route at `/api/telemetry`:

1. Requests must include the same `Authorization: Bearer <token>` header issued
   by `/api/access`; unauthenticated requests are rejected with 401.
2. When `TELEMETRY_LOG_ONLY=true`, the Worker writes each event to structured
   logs (visible via `wrangler tail`) and returns immediately. This is the
   default behaviour for development.
3. When `TELEMETRY_R2_BUCKET` is defined, batches are persisted to the named R2
   bucket with one object per upload. Objects are stored under
   `<ymd>/<uuid>.jsonl` so they can be processed later with analytics tooling.

No raw telemetry is stored in the browser; once the flush promise resolves the
queue is cleared.

## Inspecting telemetry output

Use the following approaches to confirm telemetry delivery end-to-end:

- **Local development** – Run `wrangler dev --config worker/wrangler.toml` and
  inspect console output; the Worker prints the decoded batch when
  `TELEMETRY_LOG_ONLY` is true.
- **Tail production traffic** – Execute `wrangler tail` with the production
  environment bindings to stream structured logs from the live Worker.
- **Review persisted batches** – If R2 storage is enabled, list objects with
  `wrangler r2 object list <bucket>` and download the relevant JSONL files for
  offline analysis.

Refer back to [docs/access-gate.md](access-gate.md#telemetry-and-token-reuse)
for guidance on reusing the access gate token and rotating credentials.
